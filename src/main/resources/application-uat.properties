# Web Config
server.port=8082

# MySQL Datasource
spring.datasource.url = jdbc:mysql://127.0.0.1:3306/test?useSSL=false&useUnicode=true&characterEncoding=UTF-8
#账号密码为虚拟
spring.datasource.username = root
spring.datasource.password = Ruan1991!
spring.datasource.driverClassName = com.mysql.jdbc.Driver
spring.datasource.max-active=50
spring.datasource.max-idle=20
spring.datasource.initial-size=10

# JPA config
spring.jpa.database-platform=org.hibernate.dialect.MySQLDialect
spring.jpa.hibernate.ddl-auto=none
spring.jpa.database=mysql
spring.jpa.show-sql=true

#使用com.googlecode.log4jdbc可以让Hibernate打印sql的时候显示参数和执行时间
#spring.datasource.url = jdbc:log4jdbc:mysql://localhost:3306/test?useSSL=false&useUnicode=true&characterEncoding=UTF-8
#spring.datasource.driverClassName = net.sf.log4jdbc.DriverSpy


#redis config
#注意：spring-boot-starter-data-redis新版区分两个不同的实现，jedis及lettuce,默认是使用lettuce,redis连接池需要引入commons-pool2类库
# Redis数据库索引（默认为0）
spring.redis.database=0
# Redis服务器地址
spring.redis.host=127.0.0.1
# Redis服务器连接端口
spring.redis.port=6379
# Redis服务器连接密码（默认为空）
spring.redis.password=
# 连接池最大连接数（使用负值表示没有限制） 默认 8
spring.redis.lettuce.pool.max-active=8
# 连接池最大阻塞等待时间（使用负值表示没有限制） 默认 -1
spring.redis.lettuce.pool.max-wait=-1
# 连接池中的最大空闲连接 默认 8
spring.redis.lettuce.pool.max-idle=8
# 连接池中的最小空闲连接 默认 0
spring.redis.lettuce.pool.min-idle=0

#=============== kafka consumer config =======================
# 指定kafka server的地址，集群配多个，中间，逗号隔开
spring.kafka.bootstrap-servers=192.168.40.136:9092

# 指定默认消费者group id --> 由于在kafka中，同一组中的consumer不会读取到同一个消息，依靠groud.id设置组名
#不使用group的话，启动10个consumer消费一个topic，这10个consumer都能得到topic的所有数据，相当于这个topic中的任一条消息被消费10次。
spring.kafka.consumer.group-id=testGroup100
#earliest：分区partition下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费,
#如果采用这种方式程序最好做到幂等，否则可能由于不小心改了consumer.group-id而导致重复消费
#latest：分区partition下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
#none：topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=latest
# enable.auto.commit:true --> 设置自动提交offset
spring.kafka.consumer.enable-auto-commit=true
#如果'enable.auto.commit'为true，则消费者偏移自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
spring.kafka.consumer.auto-commit-interval=100

#每间隔max.poll.interval.ms我们就调用一次poll,用于批处理场景
#max.poll.interval.ms = 300000
#每次poll最多返回50条记录

# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=com.alibaba.otter.canal.client.kafka.MessageDeserializer


#无意义的测试配置
spring.test.config.name=rock_uat
spring.test.config.poolSize=8

